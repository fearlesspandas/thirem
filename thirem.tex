

\documentclass[12pt]{amsart}
\usepackage{graphicx,amssymb,amsmath,algorithm2e,algorithmic}
\begin{document}


\newtheorem{thm}{\noindent\bf Theorem:}[section]
\newtheorem{lem}{\noindent\bf Lemma:}[section]
\newtheorem{cor}{\noindent\bf Corrallary:}[section]
\newenvironment{myproof}{\noindent\textbf{Proof:}}{QED }
\newcommand{\N}{\mathbb{N}}
\newcommand{\bigO}{\mathbb{O}}
\newcommand{\pyspace}{\hspace{12pt}}


\section{Introduction}

\section{Propositions as Tokens}

Modern Mathematics is very much a game played by taking a collection of statements one has accepted to be true and expanding that collection by applying a subset of these statements as protocols for generating new ones. While this can be modeled, in a sense, by specifying rules for the manipulation of characters in a lexicon, this is not an accurate representation of the whole picture of what mathematics inherently is. Further, translating mathematics in this way from a computational perspective is possible, but removes much of what makes mathematics interesting to the human observer. In affect we would like to produce a model that can generate new statements in a manner more like how they are used in proper proofs. Mathematicians do this by applying exceedingly powerful statements that are rich enough in their built up language that encapsulate a morphism much more powerful than mere character manipulation. In the domain of knowledge base design, which are prototypically based on the former approach, the heuristic approach to solving this problem is to prioritize inference rules that have been generated recently expecting their expressive power to increase with steps taken. In general this heuristic approach is quite limited. Locally over a small or very homogenous knowledge base this may be practical, however if one aims to represent the whole, or a significant portion, of all mathematical knowledge this does almost nothing. This is due to the fact that different fields often have different foundations, and in order to make very broad connections between fields we have to occaisionally prioritize less powerful statements for the purpose of generating a new theory.

What we suggest here is instead a symbiotic system between us and the knowledge base, and view the application of inference rules as more of an economic event. To do this we take the paradigm of "propositions as types", first described by Vovoedsky et al, with the modification that we model types as tokens of exchange.

A token $T = A\cup D$ is defined to be a collection of propositions, of which there are assertions/assumptions in $A$ and derived statements in $D$. Every derived statement has associated with it a proof. To add a new derived statement $s$ a sequence of propositions in $T$ must be given that formulates the statement $s$. This sequence is what we refer to as the proof of $s$. Each proof also naturally is associated with the necessary base tokens required to prove it in the sense that all other statements up until our desired proposition are generated from an initial collection of tokens. Also a token can itself have a proof, as we could simply define its proof to be the proofs of each of its derived statements. We can carry this paradigm forward and also assume that propositions are actually the same thing as tokens, in the sense that whenever we have a proposition $s$ it naturaly maps onto a token $T'$ with the only assertion being $s$. In this sense whenever we are proving theorems we are actually mining tokens. Transacting these tokens then passes on the building blocks for proving a proposition. However, if the new owner of a token does not wish to verify the proof they do not have to, which is the main utility of a transaction.

One can imagine that great collections of results in a theory would innevitably be a sort of master token in its domain.



Whenever a tokens requirements of any inference rule are met by another token, i.e. token $T$ has produced the required tokens to execute a mining step, that step is performed and new tokens mined to whoever hosts the event.



\section{Theorem Proposition,the Sharding Problem, and the Uncorrupted Genesis Axiom}

\subsection{How Propositions as Tokens Effects Decentralization}

At the core of decentralized theory lies the motivation for the Sharding Problem which fundamentally asks what the limits of a decentralized system are. The Sharding Problem (SP) asks whether a distributed network protocol can be designed that does not require every node on the network to have a complete copy of the blockchain, and still maintain trustlessness. Having this requirement limits the capacity of the network as the network as a whole can only remain simple/small enough to be managed on the weakest node. Ideally we would like the combination of many nodes to be able to describe more complexity as a whole than any individual participator. There have been proposals such as \textbf{plasma} that allows for retroactive fixes to be made to the network when necessary, allowing most nodes to not process all transactions. However, there has been an apparent lack of work specifying in what instances such protocols are even necessary. More importantly the way in which the complexity of the total network is scaled seems like a somewhat naive attempt when examining the capacity of the network in the perspective of finite model theory. The paradigm described in section 2 does in fact provide a natural solution to sharding as well as a more informative classification of network capacity in the following way. First we no longer view the network in most cases as being the one ethereum network, but instead view it as an ecosystem of protocols each of which is in itself a network. We describe a token that encapsulates our network protocol, and mandate that every new actor on the network obtains this token from an already participating member. More importantly can we do this so that owning a token $A$ is then equivalent to proof that the ownership of $A$ is proof itself of a proper protocol. By this we mean more mathematically that the only tokens $\{T_i\}$ which have any mapping into $A$ only produce valid mappings into $A$ i.e. we assert if we have $A$ then we must also have some $\{T_j\}\subseteq \{T_i\}$. In general what we want is the assertion that the only way to obtain $A$ through legitamate means is to definitely also have the necessary building blocks over $\{T_i\}$. Once this is established, as we will see it is then in many instances possible to verify a proposition with minimal resources much less than the full ethereum network. We will establish that using this design paradigm we do indeed need at least one full node over our network constructed from $\{T_i\}$ in order to validate all types of propositions about $\{T_i\}$. But, what is then shown is, first, that really \textbf{only} one full node is actually necessary to do this while for some problems a full node is never required, and moreover what we consider a full node can be further discretized down into a natural collection of partial nodes. The first natural observation we then make from this is that describing the capacity of the network is integrally related to the protocol for the network, with the important distinction being whether or not our network is trying to prove \textbf{existential} propositions or \textbf{universal} ones. An interesting correllary from this is that if our protocol is designed to be a mathematical, monotonic language, then in general verification of a token from just one node is in fact enough to prove its validity with certainty.  \newline

Verifying an event on chain then comes down to testing ownership of a token that proves the event, where the only way to obtain such a token was to take part in the proper mining protocol. If we allow for propositions to possibly be absorbed in the mining step as part of a tokens protocol we can then incorporate more than just mathematical logic but also non-monatonic logics.\newline

A transaction $t:A\rightarrow B$ for example could be "mined" as such. Let $P$ be a protocol token that only produces valid mappings to $t$. First, obtain a protocol token $P$ from a node that already has it. Next, mine $t$ or a statement $s$ that proves $t$, using $P$. This token $s$ can then be sent across the network to any other node that wants proof of the transaction. If we allow for base tokens to be absorbed in the protocol of $P$, then, for example, we can ensure that if $B$ spends its funds from $A$, $s$ must be used in the mining step and therefore cannot be double spent. Observe that $P$ produces type $t$ as intended, hence we know that $s$ must then have been absorbed in the production of $t$ therefore any nodes that withness $t$ can also trust that $B$ no longer has ownership of $s$.

\subsection{Formalizing Propositions as Tokens and the Mining Step}

Assume for the moment that when we initiate our token network $L$ the first nodes participating in the network via token $L$ are in fact set up properly to be \textbf{Uncorrupted}, i.e. all tokens generated from $L$ are valid in the sense described above. Specifically we want all tokens derived from $L$ to contain $L$ as a possible requirement for being considered a legitimate mining step. We call this assumption the \textbf{Uncorrupted Genesis Axiom}. \newline


Our token $L$ is then composed of $A$ assumptions and $D$ derived statements. Suppose token $t$ can be derived from $L$. We then define token $t$ to have the property that it contains a proposition $\phi$ where $\phi$ is the boolean value associated with the validity of the statement "$\forall \psi\in t_A , \psi\in L$". By this we mean that $L$, or at the very least the node that is processing the validity of $t$, has access to all assumptions in $t$. We may refer to $\phi$ as the $\textbf{validity check}$ of $t$, $t_\phi$.

\begin{thm}
 If $t$ is generated from an uncorrupted $L$ and has $t_\phi = True$ then for all statements tokens $s$ such that $s$ is absorbed in the production of $t$, then any node $N_1$ that witnesses $t$ also witnesses the desctruction of $s$ from some $N_2$.
\end{thm}
\begin{proof}
By the assumption that $L$ is uncorrupted, we know that to produce a valid $t$ our desired protocol is being run, i.e. if we simply assume that any mapping of $L\mapsto t$ asserts $t_\phi$ then $L$ must have access to all of $t_A$ on some collection of nodes $\{N_i\}$. If $s$ is an assumption of $t$ for which an uncorrupted node would absorb $s$, then we know $L$ loses $s$ in the production of $t$ taken from some member of $\{N_i\}$.
\end{proof}

Broadening our scope a bit, what this means is potentially we can choose to not worry about incongruences in the network for certain types of problems. Observe first of all that if we have a monotonic logic, i.e. a logic where adding new statements does not negate any of our previous theorems, then we naturally might not care about double spending at all. The proposition $2+2 = 4$ does not require absorbtion in many contexts. If I wanted to use this statement in a proof for another statement $4 + 4 = 8$ the fact that $2 +2 = 4$ does not change. This inherently is because with mathematical statements in order to prove a theorem we only need verification that there exists a mining sequence which validly produces it. This is equivalent to just mining the token with the correct protocol, hence only one node is required to provide adequate proof. \newline

However, consider a non-monotonic logic such as a payment system, where the statement "$A$ has balance $x$" can be proven, and then later disproven. In this case if we consider only existential statements expressible as tokens, this collection of tokens only requires the capacity of one node to be mined. In contrast, statements with universal quantification will often require the power of a full node (only up to all nodes running our protocol/token set) to mine.

Let $\{L_i\}$ be a collection of protocol tokens. We say we have a network on $L_i$ if we assume our node has access to tokens minable from $L_i$. A \textbf{full node} of $L_i$ is a node that witnesses all tokens minable from $L_i$.

\begin{lem}
For any token $t$ that is only minable from $\{L_i\}$, if $t$ requires a universal proof over token $s$ (i.e. a statement a mining step requiring a token of $\forall s, p(s)$) that is only minable from $\{L_i\}$, then any node $M$ that runs a full node of $\{L\}$ can witness this proof.
\end{lem}
\begin{proof}
The proof is somewhat trivial. By the fact that we are running a full node of network $\{L_i\}$, we know all instances of $s$ up to some amount of propogation delay.
\end{proof}

\begin{cor}
Let $L_i$ be a unique collection of tokens in the sense that any set of tokens $S_i$ on our network that mines tokens minable from $L_i$ must have $S_i\subseteq L_i$. Then only one full node over uncorrupted token $\{L_i$\} is enough for all nodes on the network to mine all tokens minable from $\{L_i\}$ with validity.
\end{cor}


\end{document}
