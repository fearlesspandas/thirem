

\documentclass[12pt]{amsart}
\usepackage{graphicx,amssymb,amsmath,algorithm2e,algorithmic}
\begin{document}


\newtheorem{thm}{\noindent\bf Theorem:}[section]
\newtheorem{lem}{\noindent\bf Lemma:}[section]
\newtheorem{cor}{\noindent\bf Corrallary:}[section]
\newenvironment{myproof}{\noindent\textbf{Proof:}}{QED }
\newcommand{\N}{\mathbb{N}}
\newcommand{\bigO}{\mathbb{O}}
\newcommand{\pyspace}{\hspace{12pt}}


\section{Introduction}


\section{Propositions as Tokens}
A \textbf{token} $T = A\cup D$ is defined to be a collection of propositions, of which there are assertions/assumptions in $A$ and derived statements in $D$. Every derived statement has associated with it a proof. To obtain a new derived statement $s$ a sequence of propositions in $T$ must be found that formulates the statement $s$. This sequence is what we refer to as the proof of $s$. Each proof also naturally is associated with the necessary base tokens required to prove it in the sense that all other statements up until our desired proposition are generated from an initial collection of tokens. Also a token can itself have a proof, as we could simply define its proof to be the proofs of each of its derived statements. We can carry this paradigm forward and also assume that propositions are actually the same thing as tokens, in the sense that whenever we have a proposition $s$ it naturaly maps onto a token $T'$ with the only assertion being $s$. \newline

In this sense whenever we are proving theorems we are actually mining tokens. Transacting these tokens then passes on the building blocks for proving a proposition. As we will establish in detail later, we can use this paradigm of \textbf{proposotions as tokens} to establish a system in which one can only produce a token via the valid application of inference rules over some grammar. The key insight that allows this paradigm to actually be achievable, is that all statements derivable from this grammar must be semantically isolated from the rest of the network. Intuitively this means upon constuction of a token, we make the assertion that it is only valid if its necessary conditions are met. We then only have to concern ourselves with producing good grammars and, more importantly, defining what exactly our necessary conditions are.

One can imagine that great collections of results in a theory would innevitably be a sort of master token in its domain.


\section{Theorem Proving,the Sharding Problem, and the Uncorrupted Genesis Axiom}

\subsection{How Propositions as Tokens Effects Decentralization}

At the core of decentralized theory lies the motivation for the Sharding Problem which fundamentally asks what the limits of a decentralized system are. The Sharding Problem (SP) asks whether a distributed network protocol can be designed that does not require every node on the network to have a complete copy of the blockchain, and still maintain trustlessness. Having this requirement limits the capacity of the network as the network as a whole can only remain simple/small enough to be managed on the weakest node. Ideally we would like the combination of many nodes to be able to describe more complexity as a whole than any individual participator. There have been proposals such as \textbf{plasma} that allows for retroactive fixes to be made to the network when necessary, allowing most nodes to not process all transactions. However, there has been an apparent lack of work specifying in what instances such protocols are even necessary. More importantly the way in which the complexity of the total network is measured seems like a somewhat naive attempt when examining the capacity of the network in the perspective of \textbf{finite model theory}.\newline

 The paradigm described in section 2 does in fact provide a natural solution to sharding as well as a more informative classification of network capacity in the following way. First we no longer view the network in most cases as being the one ethereum network, but instead view it as an ecosystem of protocols each of which is in itself a network. We describe a token that encapsulates our network protocol, and mandate that every new actor on the network obtains this token from an already participating member. More importantly can we do this so that owning a token $A$ is then equivalent to proof that the ownership of $A$ is proof itself of a proper protocol. By this we mean more mathematically that the only tokens $\{T_i\}$ which have any mapping into $A$ only produce valid mappings into $A$ i.e. we assert if we have $A$ then we must also have some $\{T_j\}\subseteq \{T_i\}$. In general what we want is the assertion that the only way to obtain $A$ through legitamate means is to definitely also have the necessary building blocks over $\{T_i\}$. Once this is established, as we will see it is then in many instances possible to verify a proposition with minimal resources that amount to much less than the full ethereum network.\newline

 We will establish that using the design paradigm of "propositions as tokens" we do indeed need at least one full node over our network constructed from $\{T_i\}$ in order to validate all types of propositions about $\{T_i\}$. But, what is then shown is, first, that really \textbf{only} one full node is actually necessary to do this while for some problems a full node is never required, and moreover what we consider a full node can be further discretized down into a natural collection of partial nodes. The first natural observation we then make from this is that describing the capacity of the network is integrally related to the protocol for the network, with the important distinction being whether or not our network is trying to prove \textbf{existential} propositions or \textbf{universal} ones. An interesting correllary from this is that if our protocol is designed to be a mathematical, monotonic language, then in general verification of a token from just one node is in fact enough to prove its validity with certainty.  \newline

Verifying an event on chain then comes down to testing ownership of a token that proves the event, where the only way to obtain such a token was to take part in the proper mining protocol by owning the correct set of tokens. If we allow for propositions to possibly be absorbed (or spent) in the mining step as part of a tokens protocol we can then incorporate more than just mathematical logic but also non-monatonic logics.\newline

A transaction $t:A\rightarrow B$ for example could be "mined" as such. Let $P$ be a protocol token that only produces valid mappings to $t$. First, obtain a protocol token $P$ from a node that already has it. Next, mine $t$ by providing a token (or collection ) of type $\{(t_i,v_i)\}$ where each $t_i:X_i\rightarrow A$ are some transaction that prove $A$ has the proper amount balance, and $v_i$ are a token asserting that a full node has verified each corresponding $t_i$ has not been double spent; naturally all $t_i$ would be absorbed along with $t$, so that any node running $P$ which mines $t$ also signals that $A$ has lost ownership of all $t_i$. This 

\subsection{Formalizing Propositions as Tokens and the Mining Step}

Assume for the moment that when we initiate our token network $L$ the first nodes participating in the network via token $L$ are in fact set up properly to be \textbf{Uncorrupted}, i.e. all tokens generated from $L$ are valid in the sense described above. Specifically we want all tokens derived from $L$ to contain $L$ as a possible requirement for being considered a legitimate mining step. We call this assumption the \textbf{Uncorrupted Genesis Axiom}. \newline


Our token $L$ is then composed of $A$ assumptions and $D$ derived statements. Suppose token $t$ can be derived from $L$. We then define token $t$ to have the property that it contains a proposition $\phi$ where $\phi$ is the boolean value associated with the validity of the statement "$\forall \psi\in t_A , \psi\in L$". By this we mean that $L$, or at the very least the node that is processing the validity of $t$, has access to all assumptions in $t$. We may refer to $\phi$ as the $\textbf{validity check}$ of $t$, $t_\phi$.

\begin{thm}
 If $t$ is generated from an uncorrupted $L$ and has $t_\phi = True$ then for all statements tokens $s$ such that $s$ is absorbed in the production of $t$, then any node $N_1$ that witnesses $t$ also witnesses the desctruction of $s$ from some $N_2$.
\end{thm}
\begin{proof}
By the assumption that $L$ is uncorrupted, we know that to produce a valid $t$ our desired protocol is being run, i.e. if we simply assume that any mapping of $L\mapsto t$ asserts $t_\phi$ then $L$ must have access to all of $t_A$ on some collection of nodes $\{N_i\}$. If $s$ is an assumption of $t$ for which an uncorrupted node would absorb $s$, then we know $L$ loses $s$ in the production of $t$ taken from some member of $\{N_i\}$.
\end{proof}

Broadening our scope a bit, what this means is potentially we can choose to not worry about incongruences in the network for certain types of problems. Observe first of all that if we have a monotonic logic, i.e. a logic where adding new statements does not negate any of our previous theorems, then we naturally might not care about double spending at all. The proposition $2+2 = 4$ does not require absorbtion in many contexts. If I wanted to use this statement in a proof for another statement $4 + 4 = 8$ the fact that $2 +2 = 4$ does not change. This inherently is because with mathematical statements in order to prove a theorem we only need verification that there exists a mining sequence which validly produces it. This is equivalent to just mining the token with the correct protocol, hence only one node is required to provide adequate proof. \newline

However, consider a non-monotonic logic such as a payment system, where the statement "$A$ has balance $x$" can be proven, and then later disproven. In this case if we consider only existential statements expressible as tokens, this collection of tokens only requires the capacity of one node to be mined. In contrast, statements with universal quantification will often require the power of a full node (only up to all nodes running our protocol/token set) to mine.

Let $L = \{L_i\}$ be a collection of protocol tokens. We say we have a network on $L$ if we assume our node has access to tokens minable from $L$. A \textbf{full node} of $L$ is a node that witnesses all tokens minable from $L$.

\begin{lem}
For any token $t$ that is only minable from $L$, if $t$ requires a universal proof over token $s$ (i.e. a statement/mining step requiring a token of the form $\forall s, p(s)$) that is only minable from $L$, then any node $M$ that runs a full node of $L$ can mine $t$.
\end{lem}
\begin{proof}
The proof is somewhat trivial. By the fact that we are running a full node of network $L$, we know all instances of $s$ up to some amount of propogation delay.
\end{proof}

\begin{cor}
Let $L_i$ be a unique collection of tokens in the sense that any set of tokens $S_i$ on our network that mines tokens minable from $L_i$ must have $S_i\subseteq L_i$. Then only one full node over uncorrupted tokens $\{L_i\}$ is enough for all nodes on the network to mine all tokens minable from $\{L_i\}$ with validity.
\end{cor}
\subsection{Some Examples}
First we will illustrate a payment protocol in this paradigm that is uncorrupted. Constructing such a protocol essentially comes down to constructing an attribute (type) based grammar which will serve as the mining protocol for our tokens. By convention any statements which have a $*$ next to them are absorbed in the mining step. This produces an implicit universal quantifier, in the sense that we need to verify these tokens are not being double spent, hence they can only be mined via a full node. \newline
\begin{algorithm}
$P:$\newline
$Amnt = 0$\newline
$*Amnt_u = x, *Amnt_u = y \mapsto Amnt_u = x+y$\newline

$*Amnt_A = x, *x \geq y \mapsto t_y:A\rightarrow B, Amnt_A = x - y$\newline

$*Amnt_B = x, *t_y: A\rightarrow B \mapsto Amnt_B = x + y$\newline

\end{algorithm}

In this simple payment system all transactions must be in some way verified by a full node.


\end{document}
